{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpTBUnJhV-QO"
      },
      "source": [
        "# **Aprendizaje Automático - Clasificación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqhAcnrTV-vK"
      },
      "source": [
        "- Francisco Prados Abad\n",
        "- Paola León Tarife\n",
        "- Julia de Enciso García\n",
        "- Paula Samper López\n",
        "- Camino Rodríguez Pérez-Carral\n",
        "- Lucía Yan Wu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEBkDxiqjcLs"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-07T14:46:16.764238Z",
          "start_time": "2024-10-07T14:46:16.081851Z"
        },
        "id": "huqF-TDJm93X"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import mean_absolute_error, r2_score,accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# CLasificacion\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import RadiusNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "from random import randint\n",
        "\n",
        "# Set seed\n",
        "np.random.seed(42)\n",
        "\n",
        "import joblib\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cargar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accedemos a las bases de datos procesadas\n",
        "X_train = pd.read_csv('data/clasificación/X_train.csv')\n",
        "y_train = pd.read_csv('data/clasificación/y_train.csv')\n",
        "X_test = pd.read_csv('data/clasificación/X_test.csv')\n",
        "y_test = pd.read_csv('data/clasificación/y_test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND-VO1-_KCi2"
      },
      "source": [
        "## **Modelos de clasificación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IIL4WxeeAcZ"
      },
      "source": [
        "Tras el preprocesado de los datos, se probarán distintos modelos de aprendizaje automático centrados en la tarea de clasificación.\n",
        "Previamente a la implementación de cada uno de los modelos, se analizó el objetivo y a qué tipo de conjunto de datos suele aplicarse, centrándonos en:\n",
        "-\t**Interpretabilidad** en nuestro caso, pero, generalmente en este tipo de casos de concesión de un crédito, los modelos interpretables son más valiosos con el objetivo de justificar por qué se ha aprobado o no dicho crédito (LR/DT)\n",
        "-**Comportamiento lineal/no lineal**: lineal (LR, LDA) no lineal (DT, KNN, SVC, NN). Entendemos que nuestro caso las relaciones son más complejas y no necesariamente lineales.\n",
        "-\t**Ruido y datos desbalanceados:** por ejemplo el Naive Bayes, al asumir independencia entre características, puede que no sea la mejor opción. El problema de datos desbalanceados se solventó durante el preprocesamiento.\n",
        "-\t**Cantidad de datos y dimensionalidad:** para gran volumen de datos (SGD, NN), y el KNN puede ser ineficientes con muchos datos.\n",
        "-\t**Velocidad de predicción**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tayR3m3aUnqx"
      },
      "source": [
        "### Árboles de Decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdstY_Lfet-O"
      },
      "source": [
        "El **DecisionTreeClassifier** construye un árbol de decisión para clasificar datos dividiéndolos recursivamente en subconjuntos en función de las características de entrada. Cada nodo del árbol representa una decisión basada en una característica específica, y las hojas finales representan las clases de salida.\n",
        "\n",
        "\n",
        "\n",
        "*   **max_depth:** Profundidad máxima del árbol. Controla cuántos niveles tendrá el árbol. Profundidades más grandes tienden a sobreajustar el modelo. Valores a probar: 3, 5, 7.\n",
        "*  **max_features:** Proporción de características a considerar en cada división. Valores a probar: 0.25, 0.5, 0.75, 1.0.\n",
        "*   **min_samples_leaf:** Número mínimo de muestras que debe contener una hoja. Hojas con menos muestras de las indicadas no se crearán, lo que puede evitar el sobreajuste.\n",
        "Valores a probar: 2, 3, 4, 5, 6.\n",
        "\n",
        "*   **criterion:** Función para medir la calidad de una división.\n",
        "'gini': Índice de Gini. 'entropy': Basado en la ganancia de información (entropía).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjO3YKRp2MQo",
        "outputId": "902a4f84-ef27-4388-cb9b-71e53ab7dfec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'min_samples_leaf': 2, 'max_features': 0.75, 'max_depth': 7, 'criterion': 'entropy'}\n",
            "AUC: 0.868889592099444\n"
          ]
        }
      ],
      "source": [
        "# Crear un clasificador de árbol de decisión\n",
        "arbol_decision = DecisionTreeClassifier(random_state=42, min_samples_split=12)\n",
        "\n",
        "# Definir distintos valores de los parámetros\n",
        "param_grid_tree = {\"max_depth\": [3,5,7],\n",
        "            \"max_features\": [0.25, 0.5, 0.75, 1.0],\n",
        "            \"min_samples_leaf\": [2,3,4,5,6],\n",
        "            \"criterion\": [\"gini\",\"entropy\"],\n",
        "            }\n",
        "\n",
        "arbol_decision_random = RandomizedSearchCV(estimator = arbol_decision, param_distributions = param_grid_tree, n_iter = 20, scoring = 'roc_auc', cv = 5, verbose=False)\n",
        "arbol_decision_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento\n",
        "arbol_decision_random.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones con los datos de prueba\n",
        "y_pred_tree = arbol_decision_random.predict(X_test)\n",
        "\n",
        "# Evaluar la precisión del modelo\n",
        "precision_tree = roc_auc_score(y_test, y_pred_tree)\n",
        "print(\"Mejor combinación de hiperparámetros:\", arbol_decision_random.best_params_)\n",
        "print(\"AUC:\", precision_tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXbzqQqh2MQp"
      },
      "source": [
        "### Regresión logística"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg_uqO_A2MQp"
      },
      "source": [
        "La **Regresión Logística** es un modelo lineal utilizado para problemas de clasificación binaria. Estima la probabilidad de que una instancia pertenezca a una clase, utilizando la función sigmoide para transformar la salida lineal en una probabilidad entre 0 y 1.\n",
        "\n",
        "- **penalty:** penalización regularizadora que controla la magnitud de los coeficientes. l2 = problemas lineales\n",
        "- **C:** inverso regularización, controla la magnitud de la penalización aplicada. A mayor valor de C, se reduce la regularización.\n",
        "- **solver:** optimizador. liblinear = datasets pequeños // saga = l1/elasticnet\n",
        "- **max_iter:** si el dataset es grande, conviene aumentarlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG-xrtc52MQp",
        "outputId": "882f4f14-449c-470c-e7b9-92c9ec690ed8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "35 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1204, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.98205674 0.98204991 0.98204909 0.98205312 0.98205492\n",
            " 0.97743493 0.98204991        nan 0.98141235 0.98204923        nan\n",
            " 0.98198811        nan        nan        nan 0.98204923        nan\n",
            " 0.98204947 0.98204959]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 200, 'C': 0.1}\n",
            "AUC: 0.9211876200286613\n"
          ]
        }
      ],
      "source": [
        "l_regresion = LogisticRegression(random_state=42, class_weight = 'balanced')\n",
        "\n",
        "# Definir distintos valores de los parámetros\n",
        "param_grid_lr = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],    # Varias opciones de regularización\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],      # Un rango amplio para la regularización\n",
        "    'solver': ['liblinear', 'saga'],           # Solvers compatibles con l1 y elasticnet\n",
        "    'max_iter': [100, 200, 300]                # Más iteraciones si el dataset es grande\n",
        "}\n",
        "\n",
        "l_regresion_random = RandomizedSearchCV(estimator = l_regresion, param_distributions = param_grid_lr, n_iter = 20, scoring = 'roc_auc', cv = 5, verbose=False)\n",
        "l_regresion_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "y_pred_lr = l_regresion_random.predict(X_test)\n",
        "\n",
        "# Calcular la precisión usando el AUC\n",
        "precision_lr = roc_auc_score(y_test, y_pred_lr)\n",
        "print(\"Mejor combinación de hiperparámetros:\", l_regresion_random.best_params_)\n",
        "print(\"AUC:\", precision_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdUsTDEp2MQp"
      },
      "source": [
        "### Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaZ0HHGl2MQq"
      },
      "source": [
        "**GaussianNB** es una variante del clasificador Naive Bayes que asume que las características siguen una distribución gaussiana (normal). Este modelo es rápido y efectivo para problemas de clasificación donde los datos tienen esta distribución, aunque puede no ser adecuado si las distribuciones de las características son complejas o multimodales. En este caso, no esperaríamos que devolviese buenos resultados ya que nuestros datos no siguen una distribución gaussiana, pero se probó igualmente.\n",
        "\n",
        "- **var_smoothing:** permite controlar la varianza de la suavización aplicada a evitar que las probabilidades estimadas sean exactamente cero. Para features cercanas a 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moxY4tiW2MQp",
        "outputId": "f0f61834-d12c-497b-ff04-7ca997fa8515"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 5 is smaller than n_iter=20. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Mejor combinación de hiperparámetros: {'var_smoothing': 1e-05}\n",
            "AUC: 0.8688764252227914\n"
          ]
        }
      ],
      "source": [
        "gnb = GaussianNB()\n",
        "\n",
        "# Definir los valores del hiperparámetro a buscar\n",
        "param_grid_gnb = {\n",
        "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]  # Diferentes niveles de suavización\n",
        "}\n",
        "\n",
        "# Implementar GridSearchCV con los parámetros definidos\n",
        "gnb_random = RandomizedSearchCV(estimator=gnb, param_distributions=param_grid_gnb, n_iter = 20, scoring='roc_auc', cv=5, verbose=1)\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento\n",
        "gnb_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones en los datos de test\n",
        "y_pred_gnb = gnb_random.predict(X_test)\n",
        "\n",
        "# Calcular la precisión usando el AUC\n",
        "precision_gnb = roc_auc_score(y_test, y_pred_gnb)\n",
        "print(\"Mejor combinación de hiperparámetros:\", gnb_random.best_params_)\n",
        "print(\"AUC:\", precision_gnb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG26oeLw2MQq"
      },
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su7C_FudgGmd"
      },
      "source": [
        "El clasificador **K-Nearest Neighbors (KNN)** es un modelo basado en instancias que clasifica un nuevo punto según la clase de los k vecinos más cercanos en el espacio de características. El rendimiento del modelo depende en gran medida de la elección del número de vecinos (k) y el método de distancia. De ahí que fueran uno de los principales hiperparámetros a buscar.\n",
        "\n",
        "\n",
        "*   **n_neighbors:** Número de vecinos a considerar para la clasificación. Valores a probar: 5, 10, 20, 30, 40, 50.\n",
        "\n",
        "*  **weights:** Cómo se ponderan los vecinos.'uniform': Todos los vecinos tienen el mismo peso.'distance': Los vecinos más cercanos tienen más peso.\n",
        "\n",
        "*  **algorithm:** Algoritmo para computar los vecinos más cercanos.\n",
        "'auto': Elige automáticamente el mejor algoritmo según los datos. 'ball_tree', 'kd_tree', 'brute': Diferentes estructuras de datos para búsquedas rápidas de vecinos.\n",
        "*   **leaf_size:** Tamaño de la hoja para los árboles ball_tree y kd_tree. Afecta la velocidad de construcción y consulta de árboles.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMoqSGGX2MQq",
        "outputId": "ff6b5af8-38b3-4dd7-eb65-d7d497400b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'weights': 'distance', 'n_neighbors': 20, 'leaf_size': 10, 'algorithm': 'brute'}\n",
            "AUC 0.8991934640699143\n"
          ]
        }
      ],
      "source": [
        "knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n",
        "\n",
        "# Definir distintos valores de los parámetros\n",
        "param_grid_knn = {'n_neighbors': [5, 10, 20, 30, 40, 50],\n",
        "               'weights': ['uniform', 'distance'],\n",
        "               'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "               'leaf_size': [10, 20, 30, 40, 50]\n",
        "               }\n",
        "knn_random = RandomizedSearchCV(estimator = knn, param_distributions = param_grid_knn, n_iter = 20, scoring = 'roc_auc', cv = 5, verbose=False)\n",
        "knn_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "y_pred_knn = knn_random.predict(X_test)\n",
        "\n",
        "precision_knn = roc_auc_score(y_test, y_pred_knn)\n",
        "print(\"Mejor combinación de hiperparámetros:\", knn_random.best_params_)\n",
        "print(\"AUC\", precision_knn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUfkbRKl2MQq"
      },
      "source": [
        "### SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOWb5r01hCYK"
      },
      "source": [
        "El **LinearSVC** es una implementación lineal de las máquinas de vectores de soporte (SVM). Se utiliza principalmente para problemas de clasificación binaria, buscando un hiperplano que separe las clases de manera óptima, de ahí que lo hayamos escogido en nuestro problema. Utiliza penalización L2 para regularización y admite varias funciones de pérdida.\n",
        "\n",
        "\n",
        "*   **penalty:** Regularización aplicada. Solo admite 'l2' en LinearSVC.\n",
        "\n",
        "*   **loss:** Función de pérdida.'hinge': Pérdida de margen duro (SVM clásico).'squared_hinge': Pérdida cuadrada del margen.\n",
        "*   **C:** Parámetro de regularización (inverso de la fuerza de regularización). Valores más grandes permiten menos regularización.\n",
        "Valores a probar: 0.0001, 0.001, 0.01, 0.1, 1, 10, 100.\n",
        "*   **max_iter:** Número máximo de iteraciones para la convergencia del algoritmo.Valores a probar: 1000, 5000, 10000.\n",
        "*   **tol:** Tolerancia para detener el criterio de convergencia. Valores a probar: 1e-4, 1e-3, 1e-2.\n",
        "*   **dual:** Resolver el problema dual (True) o primario (False). En datasets con muchas características, dual=False puede ser más eficiente.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz4TQ3JK2MQr",
        "outputId": "f2c86b3e-06fa-4c29-ea1c-d5a656c6be28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "10 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/svm/_classes.py\", line 317, in fit\n",
            "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\", line 1214, in _fit_liblinear\n",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\", line 1046, in _get_liblinear_solver_type\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.98112505 0.98201129 0.97957419 0.9778616  0.98202622 0.9817011\n",
            " 0.98112506 0.98201589        nan 0.98160744 0.98195485 0.97787323\n",
            " 0.98112465 0.98203542 0.9820181  0.9820181  0.98112513 0.98112517\n",
            " 0.98112513        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'tol': 0.001, 'penalty': 'l2', 'max_iter': 1000, 'loss': 'squared_hinge', 'dual': True, 'C': 0.1}\n",
            "AUC 0.9202466891290162\n"
          ]
        }
      ],
      "source": [
        "svm = LinearSVC(random_state=42, class_weight='balanced')\n",
        "param_grid_svm = {\n",
        "    'penalty': ['l2'],                           # Penalización l2 (la única disponible en LinearSVC)\n",
        "    'loss': ['hinge', 'squared_hinge'],          # Tipos de pérdida\n",
        "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], # Regularización\n",
        "    'max_iter': [1000, 5000, 10000],             # Número máximo de iteraciones\n",
        "    'tol': [1e-4, 1e-3, 1e-2],                  # Tolerancia para la convergencia\n",
        "    'dual': [True, False]                        # Resolver el problema en su forma dual o primal (para datasets con muchas features = False)\n",
        " }\n",
        "\n",
        "svm_random = RandomizedSearchCV(estimator = svm, param_distributions = param_grid_svm, n_iter = 20, scoring = 'roc_auc', cv = 5, verbose=False)\n",
        "svm_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "y_pred_svm = svm_random.predict(X_test)\n",
        "\n",
        "precision_svm = roc_auc_score(y_test, y_pred_svm)\n",
        "print(\"Mejor combinación de hiperparámetros:\", svm_random.best_params_)\n",
        "print(\"AUC\", precision_svm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-uiBjdK2MQr"
      },
      "source": [
        "### SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4bqRhWKhfGR"
      },
      "source": [
        "El **SGDClassifier** es una implementación de clasificación lineal que utiliza descenso por gradiente estocástico. Es eficiente para grandes conjuntos de datos, ya que actualiza los pesos de manera iterativa con mini-batches de datos, en lugar de procesar todo el dataset de una vez. Como nuetra dimensión es relativamente alta, se decidió probar este modelo.\n",
        "\n",
        "\n",
        "\n",
        "*  **penalty:** Regularización L2 para evitar el sobreajuste.\n",
        "\n",
        "*   **loss:** Función de pérdida.'hinge': Pérdida de margen duro (como en SVM).'squared_hinge': Pérdida cuadrada de margen (más suave).\n",
        "*   **alpha:** Parámetro de regularización (fuerza de la penalización).\n",
        "Valores a probar: 1e-6, 1e-4, 1e-3, 1e-2, 0.1, 1.\n",
        "\n",
        "*   **max_iter:** Número máximo de iteraciones para convergencia.Valores a probar: 10, 100, 1000, 5000, 10000.\n",
        "\n",
        "*   **tol:** Tolerancia para la convergencia del optimizador. Valores a probar: 1e-4, 1e-3, 1e-2.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4__d748C2MQr",
        "outputId": "45edccba-aaf4-49a1-d14d-782d082c156d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'tol': 0.001, 'penalty': 'l2', 'max_iter': 10000, 'loss': 'hinge', 'alpha': 0.001}\n",
            "AUC: 0.9217728009752576\n"
          ]
        }
      ],
      "source": [
        "### SGD\n",
        "sgd = SGDClassifier(random_state=42)\n",
        "param_grid_sgd = {\n",
        "    'penalty': ['l2'],                          # Penalización L2\n",
        "    'loss': ['hinge', 'squared_hinge'],          # Función de pérdida\n",
        "    'alpha': [1e-6, 1e-4, 1e-3, 1e-2, 0.1, 1],  # Parámetro de regularización\n",
        "    'max_iter': [10, 100, 1000, 5000, 10000],   # Número máximo de iteraciones\n",
        "    'tol': [1e-4, 1e-3, 1e-2]                   # Tolerancia para la convergencia\n",
        "}\n",
        "\n",
        "sgd_random = RandomizedSearchCV(estimator = sgd, param_distributions = param_grid_sgd, n_iter = 20, scoring = 'roc_auc', cv = 5, verbose=False)\n",
        "sgd_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Definir el clasificador SGD\n",
        "# Hacer predicciones en los datos de test\n",
        "y_pred_sgd = sgd_random.predict(X_test)\n",
        "\n",
        "# Calcular la precisión usando el AUC\n",
        "precision_sgd = roc_auc_score(y_test, y_pred_sgd)\n",
        "print(\"Mejor combinación de hiperparámetros:\", sgd_random.best_params_)\n",
        "print(\"AUC:\", precision_sgd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0PcttbA2MQr"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxf7cWECh6L7"
      },
      "source": [
        "El **Linear Discriminant Analysis (LDA)** es un clasificador lineal que busca proyectar los datos en un espacio de menor dimensión, maximizando la separación entre las clases. Es útil en problemas donde las clases son linealmente separables. En este caso, no es la caraterística más representable, aun así, se decidió probar este modelo para ver qué resultados ofrecía.\n",
        "\n",
        "\n",
        "\n",
        "*  **solver:** Método de resolución.'svd': No aplica regularización.'lsqr': Resuelve el problema de forma rápida y puede aplicar regularización.'eigen': Similar a 'lsqr', pero basado en descomposición de valores propios.\n",
        "*   **priors:** Proporciones a priori de las clases. Si no se especifica, se asume que las clases están balanceadas.\n",
        "*   **shrinkage:** Regularización aplicada a la estimación de la covarianza. Solo para 'lsqr' y 'eigen'.'auto': Selecciona automáticamente el parámetro de regularización.\n",
        "*  **store_covariance:** Si se debe almacenar la matriz de covarianza del modelo.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFMaEkzu2MQr",
        "outputId": "d95b977e-f665-4499-f630-e80abaad7209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'store_covariance': False, 'solver': 'eigen', 'shrinkage': 'auto', 'priors': [0.3, 0.7]}\n",
            "AUC: 0.8534073482799613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "20 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py\", line 629, in fit\n",
            "    raise NotImplementedError(\"shrinkage not supported with 'svd' solver.\")\n",
            "NotImplementedError: shrinkage not supported with 'svd' solver.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.97891473 0.97894565        nan 0.97894565 0.97891472\n",
            "        nan 0.97891473 0.97891473 0.97894565 0.97891473 0.97959348\n",
            " 0.97955769 0.97959348 0.97894565 0.97891472 0.97894565        nan\n",
            " 0.97891473 0.97894565]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "### LDA\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "# Definir los valores de los hiperparámetros a buscar\n",
        "param_grid_lda = {\n",
        "    'solver': ['svd', 'lsqr', 'eigen'],                 # Métodos de resolución\n",
        "    'priors': [None, [0.5, 0.5], [0.3, 0.7]],            # Proporciones a priori de las clases (útil cuando conjuntos desbalanceados)\n",
        "    'shrinkage': [None, 'auto'],                         # Regularización (solo para 'lsqr' y 'eigen')\n",
        "    'storeovariance': [True, False]                    # Almacenar la matriz de covarianza\n",
        "}\n",
        "lda_random = RandomizedSearchCV(estimator = lda, param_distributions = param_grid_lda, n_iter = 20, scoring = 'roc_auc', cv = 5, verbose=False)\n",
        "lda_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones en los datos de test\n",
        "y_pred_lda = lda_random.predict(X_test)\n",
        "\n",
        "# Calcular la precisión usando el AUC\n",
        "precision_lda = roc_auc_score(y_test, y_pred_lda)\n",
        "print(\"Mejor combinación de hiperparámetros:\", lda_random.best_params_)\n",
        "print(\"AUC:\", precision_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n97vAQtBiTv7"
      },
      "source": [
        "Finalmente, podemos apreciar que el modelo con mejor resultado es el"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c-A1KrJRYcn"
      },
      "source": [
        "## **Redes neuronales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBpn2J1fR1um"
      },
      "source": [
        "Tras evaluar los modelos de aprendizaje automático previos ajustando los mejores hiperparámetros para cada uno de ellos, procedemos a probar con la implementación de redes neuronales sencillas.\n",
        "\n",
        "Las **redes neuronales** son modelos computacionales inspirados en el cerebro humano, que consisten en múltiples capas de neuronas artificiales interconectadas. Suelen ser muy efectivas en problemas de clasificación como el nuestro. Las redes neuronales aprenden ajustando los pesos de las conexiones entre neuronas mediante un proceso iterativo de optimización llamado backpropagation.\n",
        "\n",
        "Además, son capaces de capturar patrones complejos y relaciones no lineales presentes en los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnyTa6hU2MQs"
      },
      "source": [
        "### MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lVlvNmlS5Qd"
      },
      "source": [
        "Se implementó una **Red Multiceptron (MLP)** que es un tipo de red neuronal feedforward compuesta por al menos tres capas: una capa de entrada, una o más capas ocultas, y una capa de salida.\n",
        "\n",
        "Cada neurona en una capa está conectada con todas las neuronas de la siguiente capa, lo que permite al MLP capturar **patrones no lineales complejos**.\n",
        "\n",
        "Para nuestra tarea, se probó a implementar en primer lugar, una MLP sencilla para ver cómo actuaba en comparación con los modelos de aprendizaje automático previos.\n",
        "\n",
        "Para la definición del espacio de búsqueda de los hiperparámetros, primero definimos qué realiza cada uno de ellos:\n",
        "\n",
        "\n",
        "*   **hidden_layer_sizes:** especifica la arquitectura, es decir, el número de neuronas en cada capa oculta.\n",
        "*   **activation:** la función de activación que se aplicará a las neuronas. Entre los posibles valores tenemos 'tanh' y 'relu', esta última con eficiencia en problemas no lineales.\n",
        "\n",
        "*  **solver:** especifica el algoritmo de optimización para ajustar los pesos. Las posibles opciones son 'sgd' y 'adam', siendo este último generalmente más rápido y eficaz en muchas situaciones.\n",
        "*  **alpha:** Parámetro de regularización L2 (penalización de los pesos para evitar el sobreajuste).Cuyos valores posibles fueron o 0.0001, o 0.05, entre otros que se probaron.\n",
        "* **learning_rate:** Define la tasa de aprendizaje que afecta cómo se ajustan los pesos con cada iteración. Pudiendo ser fija 'constant' o adaptativa según la iteración 'adaptative'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqqkAurBydj0",
        "outputId": "0d5c60c6-c08f-496b-ab7c-86bc124cbe44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (10, 30, 10), 'alpha': 0.0001, 'activation': 'tanh'}\n",
            "AUC: 0.9258830161850472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "### MLP\n",
        "mlp = MLPClassifier(random_state=42, max_iter=100)\n",
        "# Definir los valores de los hiperparámetros a buscar\n",
        "param_grid_mlp = {\n",
        "    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "mlp_random = RandomizedSearchCV(estimator = mlp, param_distributions = param_grid_mlp, n_iter = 20, scoring = 'roc_auc', cv = 5, verbose=False)\n",
        "mlp_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones en los datos de test\n",
        "y_pred_mlp = mlp_random.predict(X_test)\n",
        "\n",
        "# Calcular la precisión usando el AUC\n",
        "precision_mlp = roc_auc_score(y_test, y_pred_mlp)\n",
        "print(\"Mejor combinación de hiperparámetros:\", mlp_random.best_params_)\n",
        "print(\"AUC:\", precision_mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajZAgEgvXTjV"
      },
      "source": [
        "Los mejores hiperparámetros obtenidos fueron: **'adam'**, que como antes se adelantaba suele ser más eficaz. Por otro lado, una tasa de aprendizaje **adaptativa**, de esa forma, varía según la iteración ajustándose a nuestro problema.\n",
        "Por último, resaltar que la arquitectura de la MLP escogida está constituida por una red de **tres capas ocultas** con 10, 30 y 10 neuronas respectivamente, aplicándose en cada una de ellas una función **tangente hiperbólica**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9D9N3vH2MQs"
      },
      "source": [
        "### Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7RtB1qbX5W9"
      },
      "source": [
        "A continuación, visto que la MLP nos proporcionó un valor superior a todo lo anterior visto, se decidió probar e investigar otro tipos de redes mediante la API de **Keras**.\n",
        "\n",
        "Keras, una API de alto nivel para redes neuronales, permite implementar fácilmente un MLP. Se define el modelo secuencialmente añadiendo capas densas (Dense) para cada capa oculta y de salida. La optimización del modelo se realiza utilizando optimizadores como adam o sgd, y se ajustan hiperparámetros como la tasa de aprendizaje, el número de capas y neuronas, el tipo de función de activación, y otros parámetros. Keras simplifica el proceso de construir, entrenar y evaluar redes neuronales.\n",
        "\n",
        "Asimismo, se implementó también una búsqueda de hiperparámetros con **Optuna**. Optuna es una herramienta para la optimización automática de hiperparámetros. En el caso de redes neuronales implementadas con Keras, Optuna puede explorar diferentes combinaciones de hiperparámetros, como el número de neuronas en cada capa, el optimizador, la tasa de aprendizaje, y el número de capas ocultas. Optuna utiliza técnicas avanzadas de búsqueda para encontrar las configuraciones óptimas, mejorando el rendimiento del modelo en menos tiempo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpcNdqJEazoE"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUc-25ePa1bt"
      },
      "source": [
        "#### Keras simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQqrlt0q2MQs",
        "outputId": "74e34ab7-f8de-41eb-8a94-33b83a90e4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.2028\n",
            "Epoch 2/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0877\n",
            "Epoch 3/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0773\n",
            "Epoch 4/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0737\n",
            "Epoch 5/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0715\n",
            "Epoch 6/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0699\n",
            "Epoch 7/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0692\n",
            "Epoch 8/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0683\n",
            "Epoch 9/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0679\n",
            "Epoch 10/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0674\n",
            "Epoch 11/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0668\n",
            "Epoch 12/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.0666\n",
            "Epoch 13/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0664\n",
            "Epoch 14/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0663\n",
            "Epoch 15/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0659\n",
            "Epoch 16/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0657\n",
            "Epoch 17/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0653\n",
            "Epoch 18/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0657\n",
            "Epoch 19/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0653\n",
            "Epoch 20/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0651\n",
            "Epoch 21/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0652\n",
            "Epoch 22/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0647\n",
            "Epoch 23/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0646\n",
            "Epoch 24/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0645\n",
            "Epoch 25/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0646\n",
            "Epoch 26/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0643\n",
            "Epoch 27/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0639\n",
            "Epoch 28/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0642\n",
            "Epoch 29/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0638\n",
            "Epoch 30/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0635\n",
            "Epoch 31/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0636\n",
            "Epoch 32/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0636\n",
            "Epoch 33/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0630\n",
            "Epoch 34/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0628\n",
            "Epoch 35/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0630\n",
            "Epoch 36/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0619\n",
            "Epoch 37/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0621\n",
            "Epoch 38/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0623\n",
            "Epoch 39/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0621\n",
            "Epoch 40/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0617\n",
            "Epoch 41/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0621\n",
            "Epoch 42/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0613\n",
            "Epoch 43/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0616\n",
            "Epoch 44/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0617\n",
            "Epoch 45/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0612\n",
            "Epoch 46/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0612\n",
            "Epoch 47/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0611\n",
            "Epoch 48/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0609\n",
            "Epoch 49/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0608\n",
            "Epoch 50/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0604\n",
            "Epoch 51/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0603\n",
            "Epoch 52/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0603\n",
            "Epoch 53/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0607\n",
            "Epoch 54/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0602\n",
            "Epoch 55/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0601\n",
            "Epoch 56/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0599\n",
            "Epoch 57/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0594\n",
            "Epoch 58/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0595\n",
            "Epoch 59/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0597\n",
            "Epoch 60/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0599\n",
            "Epoch 61/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0594\n",
            "Epoch 62/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.0594\n",
            "Epoch 63/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0594\n",
            "Epoch 64/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0591\n",
            "Epoch 65/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0589\n",
            "Epoch 66/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0592\n",
            "Epoch 67/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0593\n",
            "Epoch 68/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0595\n",
            "Epoch 69/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0591\n",
            "Epoch 70/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0590\n",
            "Epoch 71/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0589\n",
            "Epoch 72/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0589\n",
            "Epoch 73/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0586\n",
            "Epoch 74/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0583\n",
            "Epoch 75/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0583\n",
            "Epoch 76/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0585\n",
            "Epoch 77/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0581\n",
            "Epoch 78/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0585\n",
            "Epoch 79/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0584\n",
            "Epoch 80/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0582\n",
            "Epoch 81/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0579\n",
            "Epoch 82/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0576\n",
            "Epoch 83/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0576\n",
            "Epoch 84/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0580\n",
            "Epoch 85/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0578\n",
            "Epoch 86/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0577\n",
            "Epoch 87/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0574\n",
            "Epoch 88/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0573\n",
            "Epoch 89/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0575\n",
            "Epoch 90/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0571\n",
            "Epoch 91/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0575\n",
            "Epoch 92/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0573\n",
            "Epoch 93/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0571\n",
            "Epoch 94/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0578\n",
            "Epoch 95/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0572\n",
            "Epoch 96/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0573\n",
            "Epoch 97/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0571\n",
            "Epoch 98/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0571\n",
            "Epoch 99/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0568\n",
            "Epoch 100/100\n",
            "\u001b[1m1759/1759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.0573\n",
            "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Mean Absolute Error for y_pred: 0.9386934100344205\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the Neural Network model\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
        "    layers.Dense(64, activation='relu'),  # Hidden layer\n",
        "    layers.Dense(32, activation='relu'),  # Another hidden layer\n",
        "    layers.Dense(1)  # Output layer\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)  # Adjust epochs and batch_size as needed\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print(f\"AUC for y_pred: {auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99HR6TLa734"
      },
      "source": [
        "Con una implementación más sencilla, vemos que supera a la MLP anteriormente implementada, por lo tanto, se aspira a encontrar una mejor red neuronal al utilizar Optuna para encontrar los mejores hiperparámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX3FchKAbs0L"
      },
      "source": [
        "### Keras + Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrjtiXEgqBEY",
        "outputId": "6ae18ca9-e7d4-4ca5-c143-886d689a1e22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/362.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m358.4/362.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/233.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LTz5_VXzH2Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from optuna.samplers import TPESampler\n",
        "import random\n",
        "\n",
        "def set_random_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKCFZm9vbx-l"
      },
      "source": [
        "Para utilizar Optuna necesitamos un conjunto de validación, para ello, se extrae el 20% de los datos del conjunto de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrbsrRqQVzbu"
      },
      "outputs": [],
      "source": [
        "# División en conjuntos de train, test y validación\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state= 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J7AbTASb8e9"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKtZU5eLb-Jv"
      },
      "source": [
        "La siguiente configuración propone a Optuna elegir el mejor número de capas ocultas, así como el número de neuronas de cada una de ellas.\n",
        "Así mismo, se decide buscar la tasa de aprendizaje de los distintos regularizadores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igD5hCDDmiM-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_model(trial):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1,3)\n",
        "\n",
        "    input_shape = (X_train.shape[1],)\n",
        "    model.add(Input(shape=input_shape))\n",
        "\n",
        "    for i in range(num_hidden_layers):\n",
        "        units_i = trial.suggest_categorical(f'units_{i+1}', [2**i for i in range(4, 8)])\n",
        "        model.add(Dense(units_i, activation='relu', kernel_initializer='random_normal', kernel_regularizer=regularizers.L2(trial.suggest_float(f'lr_l2_{i+1}',  1e-5, 1e-2, log=True)), bias_regularizer=regularizers.L2(trial.suggest_float(f'lr_l2_{i+1}',  1e-5, 1e-2, log=True)),activity_regularizer=regularizers.L2(trial.suggest_float(f'lr_l2_{i+1}',  1e-5, 1e-2, log=True))))\n",
        "        model.add(Dropout(trial.suggest_float(f'dropout_{i+1}', 0.2, 0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer =keras.optimizers.Adam(trial.suggest_float('lr',  1e-5, 1e-2, log=True)), loss='binary_crossentropy', metrics=['auc'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwgQEAfZoDVD"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    model = create_model(trial)\n",
        "\n",
        "    model.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=False)\n",
        "\n",
        "    _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIzWT8b0oa1Z",
        "outputId": "e14420fa-f12e-4d99-c844-7f1f67e07c8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-10-19 10:57:55,232] A new study created in memory with name: no-name-cf9ddee7-0758-47b0-80a0-3b2517bafde0\n",
            "[I 2024-10-19 11:00:09,231] Trial 0 finished with value: 0.9854685068130493 and parameters: {'num_hidden_layers': 1, 'units_1': 64, 'lr_l2_1': 1.0877873896027841e-05, 'dropout_1': 0.4, 'lr': 7.127820730729079e-05}. Best is trial 0 with value: 0.9854685068130493.\n",
            "[I 2024-10-19 11:02:21,227] Trial 1 finished with value: 0.9814470410346985 and parameters: {'num_hidden_layers': 1, 'units_1': 32, 'lr_l2_1': 0.000174447948910878, 'dropout_1': 0.2, 'lr': 0.00798783466067626}. Best is trial 0 with value: 0.9854685068130493.\n",
            "[I 2024-10-19 11:05:16,981] Trial 2 finished with value: 0.9814420938491821 and parameters: {'num_hidden_layers': 3, 'units_1': 64, 'lr_l2_1': 0.00042081708994635357, 'dropout_1': 0.2, 'units_2': 64, 'lr_l2_2': 0.00010034806695704537, 'dropout_2': 0.5, 'units_3': 32, 'lr_l2_3': 0.001389143112875942, 'dropout_3': 0.5, 'lr': 0.004035121240455675}. Best is trial 0 with value: 0.9854685068130493.\n",
            "[I 2024-10-19 11:08:04,151] Trial 3 finished with value: 0.9841079711914062 and parameters: {'num_hidden_layers': 3, 'units_1': 16, 'lr_l2_1': 0.006423898272093024, 'dropout_1': 0.5, 'units_2': 128, 'lr_l2_2': 8.777271756982843e-05, 'dropout_2': 0.2, 'units_3': 64, 'lr_l2_3': 2.6581306453721024e-05, 'dropout_3': 0.4, 'lr': 0.0005402162525828042}. Best is trial 0 with value: 0.9854685068130493.\n",
            "[I 2024-10-19 11:10:25,381] Trial 4 finished with value: 0.9853614568710327 and parameters: {'num_hidden_layers': 1, 'units_1': 64, 'lr_l2_1': 8.878500734702595e-05, 'dropout_1': 0.30000000000000004, 'lr': 0.0008914820821937976}. Best is trial 0 with value: 0.9854685068130493.\n",
            "[I 2024-10-19 11:13:11,790] Trial 5 finished with value: 0.9850972890853882 and parameters: {'num_hidden_layers': 3, 'units_1': 16, 'lr_l2_1': 4.041339632298669e-05, 'dropout_1': 0.30000000000000004, 'units_2': 16, 'lr_l2_2': 0.0006613438064205661, 'dropout_2': 0.5, 'units_3': 16, 'lr_l2_3': 0.0006606506459409892, 'dropout_3': 0.2, 'lr': 0.0010599523959984765}. Best is trial 0 with value: 0.9854685068130493.\n",
            "[I 2024-10-19 11:15:57,986] Trial 6 finished with value: 0.9785202741622925 and parameters: {'num_hidden_layers': 2, 'units_1': 128, 'lr_l2_1': 3.1511189876899045e-05, 'dropout_1': 0.5, 'units_2': 128, 'lr_l2_2': 0.0006634729589541303, 'dropout_2': 0.30000000000000004, 'lr': 0.007024580602413085}. Best is trial 0 with value: 0.9854685068130493.\n",
            "[I 2024-10-19 11:19:07,500] Trial 7 finished with value: 0.9855896234512329 and parameters: {'num_hidden_layers': 2, 'units_1': 128, 'lr_l2_1': 0.00830120697589109, 'dropout_1': 0.30000000000000004, 'units_2': 32, 'lr_l2_2': 1.2162889296522105e-05, 'dropout_2': 0.5, 'lr': 1.4751006949053942e-05}. Best is trial 7 with value: 0.9855896234512329.\n",
            "[I 2024-10-19 11:22:01,720] Trial 8 finished with value: 0.9853659868240356 and parameters: {'num_hidden_layers': 3, 'units_1': 64, 'lr_l2_1': 0.0044103615005279505, 'dropout_1': 0.5, 'units_2': 128, 'lr_l2_2': 0.0010027891065672587, 'dropout_2': 0.4, 'units_3': 16, 'lr_l2_3': 8.05473381299419e-05, 'dropout_3': 0.30000000000000004, 'lr': 0.00017528021264597037}. Best is trial 7 with value: 0.9855896234512329.\n",
            "[I 2024-10-19 11:24:52,457] Trial 9 finished with value: 0.9824281930923462 and parameters: {'num_hidden_layers': 3, 'units_1': 64, 'lr_l2_1': 0.0026577599409375683, 'dropout_1': 0.30000000000000004, 'units_2': 32, 'lr_l2_2': 0.0012408295586162088, 'dropout_2': 0.2, 'units_3': 64, 'lr_l2_3': 6.418096211370863e-05, 'dropout_3': 0.5, 'lr': 0.0008755133979943927}. Best is trial 7 with value: 0.9855896234512329.\n",
            "[I 2024-10-19 11:27:44,682] Trial 10 finished with value: 0.9852880239486694 and parameters: {'num_hidden_layers': 2, 'units_1': 128, 'lr_l2_1': 0.0008972222022299278, 'dropout_1': 0.4, 'units_2': 32, 'lr_l2_2': 1.0026919628829829e-05, 'dropout_2': 0.4, 'lr': 1.82918495815739e-05}. Best is trial 7 with value: 0.9855896234512329.\n",
            "[I 2024-10-19 11:30:26,753] Trial 11 finished with value: 0.9844871759414673 and parameters: {'num_hidden_layers': 1, 'units_1': 128, 'lr_l2_1': 0.0011148000426869028, 'dropout_1': 0.4, 'lr': 1.2127800241372561e-05}. Best is trial 7 with value: 0.9855896234512329.\n",
            "[I 2024-10-19 11:33:04,594] Trial 12 finished with value: 0.9852118492126465 and parameters: {'num_hidden_layers': 2, 'units_1': 32, 'lr_l2_1': 1.1314680813776642e-05, 'dropout_1': 0.4, 'units_2': 32, 'lr_l2_2': 0.009165666497563263, 'dropout_2': 0.5, 'lr': 6.183143189897969e-05}. Best is trial 7 with value: 0.9855896234512329.\n",
            "[I 2024-10-19 11:35:46,091] Trial 13 finished with value: 0.985553503036499 and parameters: {'num_hidden_layers': 1, 'units_1': 128, 'lr_l2_1': 1.56414130949073e-05, 'dropout_1': 0.30000000000000004, 'lr': 4.6660843078452154e-05}. Best is trial 7 with value: 0.9855896234512329.\n",
            "[I 2024-10-19 11:38:34,656] Trial 14 finished with value: 0.9859735369682312 and parameters: {'num_hidden_layers': 2, 'units_1': 128, 'lr_l2_1': 0.00969753221848345, 'dropout_1': 0.2, 'units_2': 64, 'lr_l2_2': 1.008839517511501e-05, 'dropout_2': 0.30000000000000004, 'lr': 2.996387003849795e-05}. Best is trial 14 with value: 0.9859735369682312.\n",
            "[I 2024-10-19 11:41:07,169] Trial 15 finished with value: 0.9858186841011047 and parameters: {'num_hidden_layers': 2, 'units_1': 128, 'lr_l2_1': 0.009563986452878159, 'dropout_1': 0.2, 'units_2': 64, 'lr_l2_2': 1.0632274256232903e-05, 'dropout_2': 0.30000000000000004, 'lr': 2.7118116609913278e-05}. Best is trial 14 with value: 0.9859735369682312.\n",
            "[I 2024-10-19 11:43:38,138] Trial 16 finished with value: 0.9859225749969482 and parameters: {'num_hidden_layers': 2, 'units_1': 128, 'lr_l2_1': 0.002043480881561699, 'dropout_1': 0.2, 'units_2': 64, 'lr_l2_2': 3.689477346566693e-05, 'dropout_2': 0.30000000000000004, 'lr': 0.00016980507493953868}. Best is trial 14 with value: 0.9859735369682312.\n",
            "[I 2024-10-19 11:46:22,288] Trial 17 finished with value: 0.98563152551651 and parameters: {'num_hidden_layers': 2, 'units_1': 128, 'lr_l2_1': 0.0021103399612114166, 'dropout_1': 0.2, 'units_2': 64, 'lr_l2_2': 5.0077721995838605e-05, 'dropout_2': 0.30000000000000004, 'lr': 0.0002096629653164051}. Best is trial 14 with value: 0.9859735369682312.\n",
            "[I 2024-10-19 11:48:56,933] Trial 18 finished with value: 0.9857170581817627 and parameters: {'num_hidden_layers': 2, 'units_1': 128, 'lr_l2_1': 0.0025576681332326427, 'dropout_1': 0.2, 'units_2': 64, 'lr_l2_2': 3.3269175863126236e-05, 'dropout_2': 0.30000000000000004, 'lr': 0.00012147676631269588}. Best is trial 14 with value: 0.9859735369682312.\n",
            "[I 2024-10-19 11:51:18,975] Trial 19 finished with value: 0.9854015111923218 and parameters: {'num_hidden_layers': 2, 'units_1': 32, 'lr_l2_1': 0.0005578268309403054, 'dropout_1': 0.2, 'units_2': 64, 'lr_l2_2': 0.0002027842497498523, 'dropout_2': 0.2, 'lr': 0.00038168099010877954}. Best is trial 14 with value: 0.9859735369682312.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials: 20\n",
            "Best trial:\n",
            "  Value: 0.9859735369682312\n",
            "Best hyperparameters: {'num_hidden_layers': 2, 'units_1': 128, 'lr_l2_1': 0.00969753221848345, 'dropout_1': 0.2, 'units_2': 64, 'lr_l2_2': 1.008839517511501e-05, 'dropout_2': 0.30000000000000004, 'lr': 2.996387003849795e-05}\n",
            "Best scores found: 0.9859735369682312\n",
            "    num_hidden_layers: 2\n",
            "    units_1: 128\n",
            "    lr_l2_1: 0.00969753221848345\n",
            "    dropout_1: 0.2\n",
            "    units_2: 64\n",
            "    lr_l2_2: 1.008839517511501e-05\n",
            "    dropout_2: 0.30000000000000004\n",
            "    lr: 2.996387003849795e-05\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(f\"Best hyperparameters: {study.best_params}\")\n",
        "print(f\"Best scores found: {study.best_value}\")\n",
        "\n",
        "params = []\n",
        "\n",
        "for key, value in trial.params.items():\n",
        "    params.append(value)\n",
        "    print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JgT7rhAcNxO"
      },
      "source": [
        "Los parámetros anteriores fueron los mejores hiperparámetros encontrados por Optuna, siendo una red de 2 capas ocultas, cada una con 128 y 64 neuronas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-ggatu8pDxp"
      },
      "outputs": [],
      "source": [
        "best_model = Sequential()\n",
        "\n",
        "input_shape = (X_train.shape[1],)\n",
        "best_model.add(Input(shape=input_shape))\n",
        "\n",
        "best_model.add(Dense(units=128, activation='relu', kernel_initializer='random_normal',  kernel_regularizer=regularizers.L2(0.00969753221848345), bias_regularizer=regularizers.L2(0.00969753221848345),activity_regularizer=regularizers.L2(0.00969753221848345)))\n",
        "best_model.add(Dropout(0.2))\n",
        "best_model.add(Dense(units=64, activation='relu', kernel_initializer='random_normal',  kernel_regularizer=regularizers.L2(1.008839517511501e-05), bias_regularizer=regularizers.L2(1.008839517511501e-05),activity_regularizer=regularizers.L2(1.008839517511501e-05)))\n",
        "best_model.add(Dropout(0.3))\n",
        "best_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "best_model.compile(optimizer =keras.optimizers.Adam(learning_rate=2.996387003849795e-05), loss='binary_crossentropy', metrics=['auc'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG9h_lRcpGlH",
        "outputId": "df0749dd-fa08-4e50-c768-8a1445311646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 14ms/step - auc: 0.8743 - loss: 0.7678 - val_auc: 0.9800 - val_loss: 0.3576\n",
            "Epoch 2/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 4ms/step - auc: 0.9773 - loss: 0.3304 - val_auc: 0.9837 - val_loss: 0.2466\n",
            "Epoch 3/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - auc: 0.9811 - loss: 0.2485 - val_auc: 0.9846 - val_loss: 0.2145\n",
            "Epoch 4/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - auc: 0.9820 - loss: 0.2220 - val_auc: 0.9851 - val_loss: 0.1992\n",
            "Epoch 5/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - auc: 0.9822 - loss: 0.2096 - val_auc: 0.9853 - val_loss: 0.1901\n",
            "Epoch 6/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - auc: 0.9829 - loss: 0.1993 - val_auc: 0.9853 - val_loss: 0.1843\n",
            "Epoch 7/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 6ms/step - auc: 0.9832 - loss: 0.1937 - val_auc: 0.9855 - val_loss: 0.1796\n",
            "Epoch 8/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 4ms/step - auc: 0.9830 - loss: 0.1903 - val_auc: 0.9855 - val_loss: 0.1765\n",
            "Epoch 9/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - auc: 0.9827 - loss: 0.1886 - val_auc: 0.9855 - val_loss: 0.1742\n",
            "Epoch 10/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - auc: 0.9830 - loss: 0.1855 - val_auc: 0.9856 - val_loss: 0.1720\n",
            "Epoch 11/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - auc: 0.9837 - loss: 0.1810 - val_auc: 0.9857 - val_loss: 0.1701\n",
            "Epoch 12/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 5ms/step - auc: 0.9833 - loss: 0.1804 - val_auc: 0.9857 - val_loss: 0.1690\n",
            "Epoch 13/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5ms/step - auc: 0.9834 - loss: 0.1790 - val_auc: 0.9858 - val_loss: 0.1676\n",
            "Epoch 14/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3ms/step - auc: 0.9835 - loss: 0.1774 - val_auc: 0.9858 - val_loss: 0.1665\n",
            "Epoch 15/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - auc: 0.9838 - loss: 0.1756 - val_auc: 0.9858 - val_loss: 0.1656\n",
            "Epoch 16/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - auc: 0.9836 - loss: 0.1750 - val_auc: 0.9858 - val_loss: 0.1646\n",
            "Epoch 17/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - auc: 0.9840 - loss: 0.1735 - val_auc: 0.9858 - val_loss: 0.1641\n",
            "Epoch 18/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - auc: 0.9837 - loss: 0.1736 - val_auc: 0.9858 - val_loss: 0.1634\n",
            "Epoch 19/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - auc: 0.9838 - loss: 0.1721 - val_auc: 0.9858 - val_loss: 0.1629\n",
            "Epoch 20/20\n",
            "\u001b[1m4503/4503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - auc: 0.9841 - loss: 0.1712 - val_auc: 0.9859 - val_loss: 0.1620\n",
            "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "AUC: 0.9802353700121966\n"
          ]
        }
      ],
      "source": [
        "best_model.fit(X_train, y_train, batch_size = BATCH_SIZE,  validation_data = (X_val, y_val),\n",
        "               epochs=EPOCHS,\n",
        "               verbose=1)\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "precision_gradient = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC:\", precision_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb7aSVyedMtx"
      },
      "source": [
        "Como se puede comprobar, esta red neuronal de Keras obtiene el resultado más alto de todos, llegando a un AUC considerablemente alto. Por lo tanto, se ha decidido escoger este modelo para utilizarlo como modelo final en la competición."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gMstCtT2MQt"
      },
      "source": [
        "## **Ensembles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KLu6BJ34jK9"
      },
      "source": [
        "Tras evaluar los modelos de aprendizaje automático y ajustar los mejores hiperparámetros para cada uno de ellos, procedemos a probar diversas técnicas de ensembles.\n",
        "\n",
        "Los modelos **ensembles** combinan múltiples clasificadores inidviduales para mejorar tanto la precisión como la estabilidad de las predicciones. Esta combinación ayuda a mitigar los problemas asociados a los modelos individuales, como el **sesgo** y la **varianza**, generando predicciones más robustas y menos susceptibles a los errores cometidos por cada modelo.\n",
        "\n",
        "El **objetivo** es realizar pruebas con distintos modelos de ensembles, tanto  **homogéneos** como **heterogéneos**, para intentar mejorar los resultados obtenidos previamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2Cfsj7N2MQt"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import VotingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngjc-eny6tAN"
      },
      "source": [
        "### **Ensembles homogéneos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kNm78K06wz1"
      },
      "source": [
        "Los ensembles **homogéneos** son aquellos que se construyen utilizando un único tipo de modelo base para realizar las predicciones. Estos ensembles combinan las salidas de múltiples instancias del mismo clasificador, que pueden estar modificadas en su implementación o entrenadas con diferentes subconjuntos de datos.\n",
        "\n",
        "Ejemplos comunes de ensembles homogéneos incluyen técnicas como:\n",
        "\n",
        "*   **Bagging** (Bootstrap Aggregating): Genera múltiples versiones del modelo base, cada una entrenada con subconjuntos seleccionados aleatoriamente de los datos de entrenamiento y con reemplazo. Debido a esta variabilidad en los datos de entrenamiento, cada modelo es único y produce resultados diferentes. Las predicciones se combinan generalmente mediante votación o promediado, lo que contribuye a mitigar la varianza de las predicciones finales.\n",
        "\n",
        "*   **Boosting**: En contraste con el Bagging, esta técnica entrena los modelos de manera secuencial, donde cada nuevo modelo se centra en corregir los errores del anterior, lo que permite una mejora continua en el rendimiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8UKmSKP2MQt"
      },
      "source": [
        "#### **Gradient Boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvX3lKdB8LL6"
      },
      "source": [
        "El **Gradient Boosting** utiliza el árbol de decisión como modelo base. Esta técnica se basa en el entrenamiento secuencial de múltiples modelos débiles, donde cada nuevo modelo se centra en corregir los errores cometidos por el anterior. De esta manera, se busca mejorar continuamente el rendimiento del ensemble al abordar las deficiencias de los modelos previos.\n",
        "\n",
        "El proceso de **Gradient Boosting** comienza con un modelo inicial que realiza predicciones basadas en los datos de entrenamiento. A continuación, se calculan los errores de este modelo, conocidos como **residuos**. Basándose en estos residuos, se generan otros **modelos débiles**, que suelen ser árboles de decisión de poca profundidad, entrenados específicamente para predecir los residuos. A partir de las predicciones de estos modelos débiles, se construye un nuevo modelo que integra estas predicciones, buscando así minimizar los residuos y mejorar la precisión general del ensemble.\n",
        "\n",
        "*   **min_samples_split**: número mínimo de muestras necesarias para dividir un nodo en el árbol de decisión\n",
        "*   **min_samples_leaf**: número mínimo de muestras que debe estar presente en una hoja  del árbol.\n",
        "*   **loss**: función de pérdida que el modelo debe minimizar\n",
        "*   **n_estimators**: número de árboles que se deben entrenar en el ensemble\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twQmcnH82MQt",
        "outputId": "6e5c4f64-1ae8-4850-90b7-d51c237e7d14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "35 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of GradientBoostingClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.98287904        nan 0.99061065 0.99120779 0.97568257\n",
            " 0.97584294 0.99123028 0.99150226        nan        nan 0.99061065\n",
            "        nan 0.99123028        nan        nan 0.99047157 0.97584359\n",
            " 0.97584294 0.9914454 ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 1, 'loss': 'exponential'}\n",
            "AUC: 0.9132367807978374\n"
          ]
        }
      ],
      "source": [
        "gradient = GradientBoostingClassifier(random_state = 42)\n",
        "param_grid_gradient = {\"min_samples_split\": [1,2,3,4,5],\n",
        "            \"min_samples_leaf\": [1,2,3,4,5],\n",
        "            \"loss\": ['log_loss','exponential'],\n",
        "            \"n_estimators\": [50,100,150,500,1000],\n",
        "            }\n",
        "\n",
        "gradient_random = RandomizedSearchCV(estimator = gradient, param_distributions = param_grid_gradient, n_iter = 20, scoring = 'roc_auc', cv = 5, verbose=False)\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento\n",
        "gradient_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones con los datos de prueba\n",
        "y_pred_gradient = gradient_random.predict(X_test)\n",
        "\n",
        "# Evaluar la precisión del modelo\n",
        "precision_gradient = roc_auc_score(y_test, y_pred_gradient)\n",
        "print(\"Mejor combinación de hiperparámetros:\", gradient_random.best_params_)\n",
        "print(\"AUC:\", precision_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMDry5d52MQt"
      },
      "source": [
        "#### **AdaBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teShcC5pEPC7"
      },
      "source": [
        "El **AdaBoost** (Adaptive Boosting) utiliza el árbol de decisión como modelo base. Esta técnica se centra en el entrenamiento secuencial de múltiples modelos, los cuales se entrenan sobre diferentes versiones del conjunto de datos de entrenamiento.\n",
        "\n",
        "El proceso de **AdaBoost** inicia con un modelo base que realiza predicciones sobre el conjunto de entrenamiento. A continuación, se calculan los errores de este modelo y se ajustan los pesos de las muestras en función de esos errores. Se aumenta el peso de las muestras que han sido clasificadas erróneamente, lo que permite que los modelos siguientes se centren en corregir estas instancias difíciles. Este enfoque iterativo ayuda a mejorar la precisión del ensemble al abordar las deficiencias de los modelos previos.\n",
        "\n",
        "*   **n_estimators**: número de árboles que se deben entrenar en el ensemble\n",
        "*   **learning_rate**: tasa de aprendizaje\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnAtNUTg2MQt",
        "outputId": "b2af36c0-5cc2-4461-89a4-d9d7d6107211"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'n_estimators': 1000, 'learning_rate': 1}\n",
            "AUC AdaBoost: 0.9154635707944117\n"
          ]
        }
      ],
      "source": [
        "ada = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de parámetros\n",
        "param_grid_ada = {\n",
        "    \"n_estimators\": [50, 100, 150, 500, 1000],\n",
        "    \"learning_rate\": [0.01, 0.1, 0.5, 1]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV para AdaBoost\n",
        "ada_random = RandomizedSearchCV(estimator=ada, param_distributions=param_grid_ada, n_iter=20, scoring='roc_auc', cv=5, verbose=False)\n",
        "\n",
        "# Entrenar el modelo\n",
        "ada_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_ada = ada_random.predict(X_test)\n",
        "\n",
        "# Evaluar precisión\n",
        "precision_ada = roc_auc_score(y_test, y_pred_ada)\n",
        "print(\"Mejor combinación de hiperparámetros:\", ada_random.best_params_)\n",
        "print(\"AUC AdaBoost:\", precision_ada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSvDTniF2MQu"
      },
      "source": [
        "#### **Bagging**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYDDultOGxGV"
      },
      "source": [
        "El **Bagging** (Bootstrap Aggregating) es una técnica de ensemble que se basa en la creación de múltiples modelos a partir de un modelo base, en este caso un árbol de decisión. Este método se centra en reducir la varianza al combinar las predicciones de varios modelos independientes, cada uno entrenado en subconjuntos aleatorios del conjunto de datos original.\n",
        "\n",
        "El proceso de **Bagging** comienza seleccionando aleatoriamente múltiples subconjuntos del conjunto de datos de entrenamiento, utilizando un **muestreo con reemplazo**. Cada uno de estos subconjuntos se utiliza para entrenar un modelo independiente. Como resultado, cada modelo aprende de diferentes variaciones de los datos, lo que permite capturar distintos patrones.\n",
        "\n",
        "Una vez que todos los modelos han sido entrenados, sus predicciones se combinan, generalmente mediante votación. Este enfoque ayuda a mitigar el sobreajuste, proporcionando una predicción más robusta y estable.\n",
        "\n",
        "*   **n_estimators**: número de árboles que se deben entrenar en el ensemble\n",
        "*   **max_samples**: proporción del conjunto de datos de entrenamiento que se utilizará para entrenar cada modelo base\n",
        "*   **max_features**: proporción de características que se utilizarán para entrenar cada modelo base\n",
        "*   **boostrap**: muestreo con reemplazo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8qXATE-2MQu",
        "outputId": "35f8c699-9198-42bf-931a-5e6767e6b38a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'n_estimators': 200, 'max_samples': 0.7, 'max_features': 0.5, 'bootstrap': False}\n",
            "AUC Bagging: 0.8934655063008513\n"
          ]
        }
      ],
      "source": [
        "bagging = BaggingClassifier(random_state=42)\n",
        "\n",
        "# Definir la grilla de parámetros\n",
        "param_grid_bagging = {\n",
        "    \"n_estimators\": [10, 50, 100, 200],\n",
        "    \"max_samples\": [0.5, 0.7, 1.0],\n",
        "    \"max_features\": [0.5, 0.7, 1.0],\n",
        "    \"bootstrap\": [True, False]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV para Bagging\n",
        "bagging_random = RandomizedSearchCV(estimator=bagging, param_distributions=param_grid_bagging, n_iter=20, scoring='roc_auc', cv=5, verbose=False)\n",
        "\n",
        "# Entrenar el modelo\n",
        "bagging_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_bagging = bagging_random.predict(X_test)\n",
        "\n",
        "# Evaluar precisión\n",
        "precision_bagging = roc_auc_score(y_test, y_pred_bagging)\n",
        "print(\"Mejor combinación de hiperparámetros:\", bagging_random.best_params_)\n",
        "print(\"AUC Bagging:\", precision_bagging)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkHeT-uj2MQu"
      },
      "source": [
        " **Baggging + MLP**: Se realizó una prueba adicional utilizando como modelo base el mejor clasificador individual obtenido, que en este caso es el MLP. Para esta prueba, se repitió la Randomized Search con los mismos parámetros. Sin embargo, los resultados obtenidos no lograron superar el rendimiento del modelo individual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4qdDFWbydj2",
        "outputId": "6e7ac3fd-de72-4aba-a9e9-84e5ddb225af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'n_estimators': 50, 'max_samples': 0.7, 'max_features': 1.0, 'bootstrap': False}\n",
            "AUC Bagging: 0.924753718335738\n"
          ]
        }
      ],
      "source": [
        "mlp = MLPClassifier(solver = 'adam', activation = 'relu', hidden_layer_sizes= (20,), alpha = 0.05, learning_rate='adaptive')\n",
        "bagging = BaggingClassifier(estimator = mlp, random_state=42)\n",
        "# Definir la grilla de parámetros\n",
        "param_grid_bagging = {\n",
        "    \"n_estimators\": [10, 50, 100, 200],\n",
        "    \"max_samples\": [0.5, 0.7, 1.0],\n",
        "    \"max_features\": [0.5, 0.7, 1.0],\n",
        "    \"bootstrap\": [True, False]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV para Bagging\n",
        "bagging_random = RandomizedSearchCV(estimator=bagging, param_distributions=param_grid_bagging, n_iter=20, scoring='roc_auc', cv=5, verbose=False)\n",
        "\n",
        "# Entrenar el modelo\n",
        "bagging_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_bagging = bagging_random.predict(X_test)\n",
        "\n",
        "# Evaluar precisión\n",
        "precision_bagging = roc_auc_score(y_test, y_pred_bagging)\n",
        "print(\"Mejor combinación de hiperparámetros:\", bagging_random.best_params_)\n",
        "print(\"AUC Bagging:\", precision_bagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9RAgIzVRKF6"
      },
      "source": [
        "#### **Random Forest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZA9nJiSJFUG"
      },
      "source": [
        "El **Random Forest** es una técnica de ensemble que combina múltiples árboles de decisión entrenados de manera indepenediente. Esta metodología se basa en el principio de Bagging.\n",
        "\n",
        "El proceso de **Random Forest** comienza generando varios árboles de decisión a partir de diferentes muestras del conjunto de entrenamiento. Estas muestras se obtienen mediante un muestreo aleatorio con reemplazo. Además, durante la creación de cada árbol, se selecciona aleatoriamente un subconjunto de características para determinar el mejor split en cada nodo, lo que añade aún más diversidad entre los árboles.\n",
        "\n",
        "Una vez que todos los árboles han sido entrenados, sus predicciones se combinan, generalmente mediante votación. Este enfoque de agregación permite que **Random Forest** capte patrones complejos en los datos mientras mantiene una mayor estabilidad y generalización en comparación con un solo árbol de decisión.\n",
        "\n",
        "*   **n_estimators**: número de árboles que se deben entrenar en el ensemble\n",
        "*   **criterion**: calidad de división de un árbol\n",
        "*   **max_samples**: proporción del conjunto de datos de entrenamiento que se utilizará para entrenar cada modelo base\n",
        "*   **max_features**: proporción de características que se utilizarán para entrenar cada modelo base\n",
        "*   **boostrap**: muestreo con reemplazo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLlyxdueRJFr",
        "outputId": "6a37bec0-e731-45c0-84e0-cdb3772c00c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "55 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "55 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\", line 433, in fit\n",
            "    raise ValueError(\n",
            "ValueError: `max_sample` cannot be set if `bootstrap=False`. Either switch to `bootstrap=True` or set `max_sample=None`.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.98560306        nan 0.98611798        nan 0.97766463 0.98461491\n",
            "        nan        nan 0.97861736        nan        nan 0.98611798\n",
            " 0.9759219         nan        nan 0.98576429 0.9777085         nan\n",
            "        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'n_estimators': 200, 'max_samples': 0.7, 'max_features': 0.7, 'criterion': 'log_loss', 'bootstrap': True}\n",
            "AUC Random Forest: 0.9102241798770256\n"
          ]
        }
      ],
      "source": [
        "random_forest = RandomForestClassifier(random_state = 42)\n",
        "\n",
        "# Lista de clasificadores a probar\n",
        "param_grid_random_forest = {\n",
        "    \"n_estimators\": [10, 50, 100, 200, 300, 500],\n",
        "    \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
        "    \"max_samples\": [0.5, 0.7, 1.0],\n",
        "    \"max_features\": [0.5, 0.7, 1.0],\n",
        "    \"bootstrap\": [True, False]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV para Bagging\n",
        "random_forest_random = RandomizedSearchCV(estimator=random_forest, param_distributions=param_grid_random_forest, n_iter=20, scoring='roc_auc', cv=5, verbose=False)\n",
        "\n",
        "# Entrenar el modelo\n",
        "random_forest_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_random_forest = random_forest_random.predict(X_test)\n",
        "\n",
        "# Evaluar precisión\n",
        "precision_random_forest = roc_auc_score(y_test, y_pred_random_forest)\n",
        "print(\"Mejor combinación de hiperparámetros:\", random_forest_random.best_params_)\n",
        "print(\"AUC Random Forest:\", precision_random_forest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrc2_iB9KW-r"
      },
      "source": [
        "### **Ensembles heterogéneos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-hEsXNpKaSI"
      },
      "source": [
        "Los **ensembles heterogéneos** son aquellos que combinan diferentes tipos de modelos para realizar las predicciones. Esto les permite aprovechar las fortalezas de cada uno de ellos. Estos modelos pueden variar en su estructura, como árboles de decisión, máquinas de soporte vectorial, regresiones logísticas, entre otros, y pueden ser entrenados en el mismo conjunto de datos o en diferentes subconjuntos.\n",
        "\n",
        "La combinación de diferentes modelos en un **ensemble heterogéneo** busca mejorar la precisión y la robustez de las predicciones, así como mitigar el sesgo presente en las predicciones de modelos individuales.\n",
        "\n",
        "En en este trabajo, se han elegido implementar dos técnicas de ensembles heterogéneos: **Stacking** y **Voting**. Los modelos seleccionados para esta combinación son aquellos que han demostrado el mejor rendimiento en los experimentos previos, junto con los parámetros óptimos obtenidos a través de la Randomized Search.\n",
        "\n",
        "*   *MLP* - 0.9258\n",
        "*   *SGD* - 0.9218\n",
        "*   *Regresión Logística* - 0.9211\n",
        "*   *SVM* - 0.9202\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCDxGXOi2MQu"
      },
      "source": [
        "####  **Stacking**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4izdCDODMUWq"
      },
      "source": [
        "**Stacking** es una técnica de ensemble que combina múltiples modelos base para mejorar la precisión de las predicciones.\n",
        "\n",
        "El proceso de **Stacking** comienza con el entrenamiento de múltiples modelos utilizando el mismo conjunto de datos de entrenamiento. Cada modelo proporciona una predicción, la cual se utiliza como entrada para un **meta modelo**. Este modelo meta, que generalmente es un clasificador más simple, se entrena para combinar las predicciones de los modelos base y mejorar así la precisión general del ensemble.\n",
        "\n",
        "Al final del proceso, el modelo meta genera la predicción final integrando las salidas de los modelos base, lo que permite, en muchos casos, superar el rendimiento de cualquier modelo individual.\n",
        "\n",
        "En este caso, se utiliza como meta modelo un Regresor Logístico y se realiza una Randomized Search para ajustar sus parámetros.\n",
        "\n",
        "*   **C**: factor de regularización\n",
        "*   **penalty**: penalización\n",
        "*   **solver**: optimizador\n",
        "*   **max_iter**: máximo de iteraciones que el algoritmo puede realizar durante el proceso de optimización\n",
        "*   **stack_method**: método que se utilizará para combinar las predicciones de los modelos base en el modelo meta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvGREh152MQu",
        "outputId": "61704e2d-ca64-438c-8573-e127a2a4e3d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "55 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 162, in _method_name\n",
            "    method_name = _check_response_method(estimator, method).__name__\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n",
            "    raise AttributeError(\n",
            "AttributeError: MLPClassifier has none of the following attributes: decision_function.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 672, in fit\n",
            "    return super().fit(X, y_encoded, sample_weight)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 242, in fit\n",
            "    self.stack_method_ = [\n",
            "                         ^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 243, in <listcomp>\n",
            "    self._method_name(name, est, meth)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 164, in _method_name\n",
            "    raise ValueError(\n",
            "ValueError: Underlying estimator mlp does not implement the method decision_function.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 672, in fit\n",
            "    return super().fit(X, y_encoded, sample_weight)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 288, in fit\n",
            "    _fit_single_estimator(self.final_estimator_, X_meta, y, fit_params=fit_params)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_base.py\", line 40, in _fit_single_estimator\n",
            "    estimator.fit(X, y, **fit_params)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 162, in _method_name\n",
            "    method_name = _check_response_method(estimator, method).__name__\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n",
            "    raise AttributeError(\n",
            "AttributeError: LinearSVC has none of the following attributes: predict_proba.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 672, in fit\n",
            "    return super().fit(X, y_encoded, sample_weight)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 242, in fit\n",
            "    self.stack_method_ = [\n",
            "                         ^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 243, in <listcomp>\n",
            "    self._method_name(name, est, meth)\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py\", line 164, in _method_name\n",
            "    raise ValueError(\n",
            "ValueError: Underlying estimator svc does not implement the method predict_proba.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.98468139        nan 0.98494365 0.95048955 0.95005701        nan\n",
            "        nan 0.98480014        nan 0.95023578 0.98460667        nan\n",
            "        nan        nan        nan 0.95024502        nan        nan\n",
            " 0.94960155        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'stack_method': 'auto', 'final_estimator__solver': 'saga', 'final_estimator__penalty': 'l2', 'final_estimator__max_iter': 100, 'final_estimator__C': 1}\n",
            "AUC Stacking: 0.9236399324839696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Definir los clasificadores base\n",
        "estimators = [\n",
        "    ('mlp', MLPClassifier(solver = 'adam', activation = 'relu', hidden_layer_sizes= (20,), alpha = 0.05, learning_rate='adaptive')),\n",
        "    ('svc', LinearSVC(C= 0.1, dual= True, loss='squared_hinge', max_iter = 10000, penalty='l2', tol = 0.0001)),\n",
        "    ('lr', LogisticRegression(C= 0.1, solver = 'liblinear', max_iter = 200, penalty='l1')),\n",
        "    ('sgd', SGDClassifier(alpha=0.001, loss = 'hinge', max_iter = 5000, penalty = 'l2', tol = 0.01))\n",
        "]\n",
        "# Inicializar el modelo Stacking\n",
        "stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "# Definir la grilla de parámetros\n",
        "param_grid_stacking = {\n",
        "    'final_estimator__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'final_estimator__penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'final_estimator__solver': ['liblinear', 'saga'],           # Solvers compatibles con l1 y elasticnet\n",
        "    'final_estimator__max_iter': [100, 200, 300],\n",
        "    'stack_method': ['auto', 'predict_proba', 'decision_function', 'predict']            # Más iteraciones si el dataset es grande\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV para Stacking\n",
        "stacking_random = RandomizedSearchCV(estimator=stacking, param_distributions=param_grid_stacking, n_iter=20, scoring='roc_auc', cv=5, verbose=False)\n",
        "\n",
        "# Entrenar el modelo\n",
        "stacking_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_stacking = stacking_random.predict(X_test)\n",
        "\n",
        "# Evaluar precisión\n",
        "precision_stacking = roc_auc_score(y_test, y_pred_stacking)\n",
        "print(\"Mejor combinación de hiperparámetros:\", stacking_random.best_params_)\n",
        "print(\"AUC Stacking:\", precision_stacking)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10muFIro2MQv"
      },
      "source": [
        "#### **Voting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RamorVswOPyH"
      },
      "source": [
        "**Voting** es una técnica de ensemble que combina las predicciones de múltiples modelos para producir una única predicción final.\n",
        "\n",
        "Existen dos enfoques principales para el voting: hard voting y soft voting. En el *hard voting*, cada modelo da un voto por la clase que predice, y la clase que recibe la mayoría de los votos se elige como la predicción final. Por otro lado, el *soft voting* combina las probabilidades asignadas a cada clase por los modelos base, eligiendo la clase con la suma más alta de probabilidades como la predicción final.\n",
        "\n",
        "*   **voting**: enfoque de combinación de predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy-tgUem2MQv",
        "outputId": "c18c02c5-6f20-4979-db38-c7bce48788f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 2 is smaller than n_iter=20. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n",
            "    response_method = _check_response_method(estimator, self._response_method)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n",
            "    raise AttributeError(\n",
            "AttributeError: VotingClassifier has none of the following attributes: decision_function, predict_proba.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n",
            "    response_method = _check_response_method(estimator, self._response_method)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n",
            "    raise AttributeError(\n",
            "AttributeError: VotingClassifier has none of the following attributes: decision_function, predict_proba.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n",
            "    response_method = _check_response_method(estimator, self._response_method)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n",
            "    raise AttributeError(\n",
            "AttributeError: VotingClassifier has none of the following attributes: decision_function, predict_proba.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n",
            "    response_method = _check_response_method(estimator, self._response_method)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n",
            "    raise AttributeError(\n",
            "AttributeError: VotingClassifier has none of the following attributes: decision_function, predict_proba.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 370, in _score\n",
            "    response_method = _check_response_method(estimator, self._response_method)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2145, in _check_response_method\n",
            "    raise AttributeError(\n",
            "AttributeError: VotingClassifier has none of the following attributes: decision_function, predict_proba.\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
            "    y_pred = method_caller(\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 211, in _get_response_values\n",
            "    y_pred = prediction_method(X)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 481, in predict_proba\n",
            "    self._collect_probas(X), axis=0, weights=self._weights_not_none\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in _collect_probas\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in <listcomp>\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                       ^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'LinearSVC' object has no attribute 'predict_proba'\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
            "    y_pred = method_caller(\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 211, in _get_response_values\n",
            "    y_pred = prediction_method(X)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 481, in predict_proba\n",
            "    self._collect_probas(X), axis=0, weights=self._weights_not_none\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in _collect_probas\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in <listcomp>\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                       ^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'LinearSVC' object has no attribute 'predict_proba'\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
            "    y_pred = method_caller(\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 211, in _get_response_values\n",
            "    y_pred = prediction_method(X)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 481, in predict_proba\n",
            "    self._collect_probas(X), axis=0, weights=self._weights_not_none\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in _collect_probas\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in <listcomp>\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                       ^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'LinearSVC' object has no attribute 'predict_proba'\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
            "    y_pred = method_caller(\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 211, in _get_response_values\n",
            "    y_pred = prediction_method(X)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 481, in predict_proba\n",
            "    self._collect_probas(X), axis=0, weights=self._weights_not_none\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in _collect_probas\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in <listcomp>\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                       ^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'LinearSVC' object has no attribute 'predict_proba'\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
            "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
            "    y_pred = method_caller(\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 211, in _get_response_values\n",
            "    y_pred = prediction_method(X)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 481, in predict_proba\n",
            "    self._collect_probas(X), axis=0, weights=self._weights_not_none\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in _collect_probas\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 456, in <listcomp>\n",
            "    return np.asarray([clf.predict_proba(X) for clf in self.estimators_])\n",
            "                       ^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'LinearSVC' object has no attribute 'predict_proba'\n",
            "\n",
            "  warnings.warn(\n",
            "/home/pvltarife/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor combinación de hiperparámetros: {'voting': 'hard'}\n",
            "AUC Voting: 0.9205737602078714\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "voting_estimators = [\n",
        "    ('mlp', MLPClassifier(solver = 'adam', activation = 'relu', hidden_layer_sizes= (20,), alpha = 0.05, learning_rate='adaptive')),\n",
        "    ('svc', LinearSVC(C= 0.1, dual= True, loss='squared_hinge', max_iter = 10000, penalty='l2', tol = 0.0001)),\n",
        "    ('lr', LogisticRegression(C= 0.1, solver = 'liblinear', max_iter = 200, penalty='l1')),\n",
        "    ('sgd', SGDClassifier(alpha=0.001, loss = 'hinge', max_iter = 5000, penalty = 'l2', tol = 0.01))\n",
        "]\n",
        "\n",
        "# Inicializar el modelo Voting\n",
        "voting = VotingClassifier(estimators=voting_estimators, voting='soft')\n",
        "\n",
        "# Definir la grilla de parámetros\n",
        "param_grid_voting = {\n",
        "    'voting': ['hard', 'soft']\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV para Voting\n",
        "voting_random = RandomizedSearchCV(estimator=voting, param_distributions=param_grid_voting, n_iter=20, scoring='roc_auc', cv=5, verbose=False)\n",
        "\n",
        "# Entrenar el modelo\n",
        "voting_random.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_voting = voting_random.predict(X_test)\n",
        "\n",
        "# Evaluar precisión\n",
        "precision_voting = roc_auc_score(y_test, y_pred_voting)\n",
        "print(\"Mejor combinación de hiperparámetros:\", voting_random.best_params_)\n",
        "print(\"AUC Voting:\", precision_voting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ8CuWcKO_Dv"
      },
      "source": [
        "### **Análisis de resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMpFx_E0PG5v"
      },
      "source": [
        "Observando los resultados, notamos que ninguno de estos modelos de ensembles logró superar el rendimiento de los mejores clasificadores individuales. Incluso al combinar técnicas como stacking y voting, los resultados no alcanzaron los niveles esperados. Esto nos demuestra que, aunque los ensembles pueden ser efectivos en muchos casos, no siempre garantizan una mejora en todos los escenarios.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKeKj11wPdW4"
      },
      "source": [
        "## **Conclusiones**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N4PsToCPiOH"
      },
      "source": [
        " En conclusión, el desempeño de los modelos de ensembles no fue comparable con el de la **red neuronal** implementada en Keras, la cual mostró un rendimiento significativamente superior a los demás modelos y técnicas utilizadas.\n",
        "\n",
        "\n",
        "\n",
        "*   Esto refuerza la idea de que, en problemas complejos, modelos más avanzados como las redes neuronales tienen la **capacidad de capturar mejor las relaciones no lineales** en los datos y producir predicciones más precisas que otros enfoques, como los árboles de decisión o las regresiones lineales.\n",
        "\n",
        "*   Al utilizar una red neuronal **sencilla y poco profunda**, con solo dos capas ocultas y técnicas como Dropout, logramos **evitar el sobreajuste**. Esto permitió mejorar la generalización y obtener mejores resultados en los datos de prueba.\n",
        "\n",
        "Sin embargo, somos conscientes de que el uso de este tipo de modelos más avanzados también conlleva algunas desventajas:\n",
        "\n",
        "*   Una desventaja importante es la **mayor complejidad** que implica entrenar redes neuronales, tanto en términos de **ajuste de hiperparámetros** como de **tiempo de entrenamiento**.\n",
        "\n",
        "\n",
        "*   Además, la red presenta una **baja interpretabilidad** en comparación con otros modelos tradicionales de aprendizaje automático, lo que puede ser un reto si se necesita explicar detalladamente las predicciones.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dEWPhX05Wt8u",
        "pcyqC3WIaMId",
        "pc38K7IZKyOH",
        "_SHW-T_qOIRd",
        "d-if2s-gGieX",
        "BJdcrV_SPSDb",
        "ZNSTBXunge56",
        "uzmum_dfX120",
        "tayR3m3aUnqx",
        "ZnyTa6hU2MQs",
        "j8UKmSKP2MQt",
        "pMDry5d52MQt"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
